{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project:  Wine Reviews & Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Members:  Gabe Thesing and Kristi Herman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "#### <u>Data Sources</u>\n",
    "#### Wine Review Data:  https://www.kaggle.com/zynicide/wine-reviews \n",
    "The base dataset that was extracted and built upon was a compilation of approxiamately 130,000 reviews of wines put together by Kaggle user zackthoutt.  This dataset was downloaded as a CSV file from Kaggle's website and imported into a Jupyter notebook for transformation. \n",
    "\n",
    "\n",
    "#### Geolocation Data for Wineries:  https://developers.google.com/maps/documentation/geocoding/intro\n",
    "The following steps were taken to retrieve and extract geolocation data from Google for wineries. The code for these extraction steps is in the <a href = \"kh_raw_data/geo_data/geo_data.ipynb\">geo_data.ipynb</a> jupyter notebook file: \n",
    "<ol><li>Imported Wine Reviews csv to a pandas dataframe.</li>\n",
    "    <li>Extracted the unique list of wineries and put into a list.</li>\n",
    "    <li>Called the Google API to get geolocation data for each winery.</li>\n",
    "    <li>Extracted the latitude and longitude from the JSON response from Google.\n",
    "    <li>Saved the latitude, longitude, and winery to a new dataframe.</li>\n",
    "    <li>Exported to a new csv file:  <a href=\"kh_raw_data/geo_data/winery-lat-lng-final.csv\">winery-lat-lng-final.csv</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "The following steps were taken to clean the data. The code for these extraction steps is in the <a href = \"ETL_final.ipynb\">ETL_final.ipynb</a> jupyter notebook file:\n",
    "1. Imported the winery-lat-lng-final.csv to a new pandas dataframe. The csv file needed to be imported with the encoding of utf8 to prevent problems loading special characters into the database.  Many winery names contain special characters like accent marks.  The latitude and longitude values were rounded to five decimal places, which is accurate to about 1 meter in geolocation data\n",
    "2. The latitude and longitude data was merged with the rest of the data before further cleaning.\n",
    "3. All rows containing NaN values in the selected columns were dropped. \n",
    "4. An index value, entitled wine_id, was assigned to each unique wine_title, to make querying multiple reviews for the same wine simple.\n",
    "5. Three pandas dataframes were organized in preparation to be loaded to MySQL as tables. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "The following steps were taken to load the data to a SQL database. The code create the database is in the <a href = \"db.sql\">db.sql</a> file.  The code to load the data from python to the SQL database is in the <a href = \"ETL_final.ipynb\">ETL_final.ipynb</a> jupyter notebook file:\n",
    "1. Created and ran the db.sql file to create a new winery_db database, with tables for location information, reviews, and a \"bottle\" table that gave identifying information about each reviewed wine. \n",
    "2. From the ETL_final.ipynb file, a connection was established to winery_db via the local port, using SQLalchemy and pymysql.\n",
    "3. The transformed data was loaded to the database with the pandas to_sql command. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "* Always remove duplicates before calling an API!!!\n",
    "* In this case, all of the response data returned from the APIs calls should have been saved out to a file or db in case there is a need to use additional data points from the responses in the future.\n",
    "* Pay attention to encoding if the data potentially contains special characters.  It needs to be the same when using the SQLalchemy engine to make a connection as it is for each table within MySQL.\n",
    "* It probably makes more sense to create schema within the ipynb file using SQLalchemy for this kind of project, instead of having a separate .sql file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ut_data",
   "language": "python",
   "name": "ut_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
